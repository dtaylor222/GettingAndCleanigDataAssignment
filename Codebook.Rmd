---
title: "Codebook For GettingAndCleaningDataAssignemnt"
author: "David Taylor - taken from the Gistfile.Rmd template"
date: "23rd August 2015"
output:
  html_document:
    keep_md: yes
---

## Project Description
The aim of the projet is to take the "UCI HAR Dataset" provided at :

https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip

and produce from it a 'tidy' data set that xtracts only the measurements on the mean and standard deviation for each measurement
then creates an independent tidy data set with the average of each variable for each activity and each subject

##Study design and data processing

###Collection of the raw data
The orignal experiment and its associated data is described in the README.txt in the "UCI HAR Dataset" directory.
What is seen in there is split into training (2947 observations) and test (7352 observations) data sets.

These observations are split into three text files (subject<suffix>.txt, y<suffix>.txt and X<suffix>.txt where <suffix> is _test or _train)
where subject is the subject, y is the activity code and X is the summary data created from the "true" raw data in the Inertial signals
subdirectory.

Each activity code is described in the activity_labels.txt file - we need his to meaningfully interpret the data.
Each of the 561 data fields per row in the 'X' data file is described in the features.txt file

##Creating the tidy datafile

###Guide to create the tidy data file
To create the tidy data file you should execute the run_analysis.R script in the same working directory that you have
extracted the "UCI HAR Dataset" directory to.

###Cleaning of the data
The script merges the test and training data sets and identifies those 'features' of the 'X' records that have either
'mean()' or 'std()' in their desriptions See the README.md document for a fuller description of the code.
The activity codes are expanded to the fuller description from the activity_labels.txt file
Then a mean is calculated for each observation type within each subject. (giving us 180 'true' observations)
This is then 'melted' into the long form that is better for analysis on the basis that each metric that we summarised
is an ovbservation in its own right

##Description of the variables in the tiny_data.txt file

### subject
Which of the thirty subjects was under test recorded as an integer (1 -30)
### activity 
Which of the six activities was being conducted
Factor w/ 6 levels  "LAYING"  "SITTING"  "STANDING" "WALKING" "WALKING_DOWNSTAIRS" "WALKING_UPSTAIRS" 
### variable
Which of the metrics in the raw data is being summarised string as per features.txt in original data
### mean
The mean value recorded for this subject / activity / metric observation (dimensionless as data normalised)